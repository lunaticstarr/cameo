{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAMEO Evaluation Test\n",
    "\n",
    "This notebook replicates the evaluation workflow from AMAS `test_LLM_synonyms_plain.ipynb` using CAMEO functions.\n",
    "It tests both single model evaluation and batch evaluation of multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import CAMEO functions\n",
    "from cameo.core import annotate_single_model, print_results\n",
    "from cameo.utils import (\n",
    "    evaluate_single_model,\n",
    "    evaluate_models_in_folder,\n",
    "    print_evaluation_results,\n",
    "    compare_with_amas_results\n",
    ")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up paths and parameters for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test model: test_models/BIOMD0000000190.xml\n",
      "Model directory: test_models\n",
      "LLM model: meta-llama/llama-3.3-70b-instruct:free\n",
      "Entity type: chemical\n",
      "Database: chebi\n",
      "Max entities per model: 10\n",
      "Number of models to test: 5\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "test_model_file = \"test_models/BIOMD0000000190.xml\"  # Single test model\n",
    "# model_dir = \"/Users/luna/Desktop/CRBM/AMAS_proj/Models/BioModels\"  # Directory with multiple models\n",
    "model_dir = \"test_models\"\n",
    "output_dir = \"./results/\"  # Output directory for results\n",
    "\n",
    "# LLM configuration\n",
    "llm_model = \"meta-llama/llama-3.3-70b-instruct:free\"  # or \"gpt-4o-mini\"\n",
    "\n",
    "# Evaluation parameters\n",
    "max_entities_per_model = 10  # Limit entities per model for testing\n",
    "num_models_to_test = 5  # Number of models to test in batch evaluation\n",
    "\n",
    "# Entity and database configuration\n",
    "entity_type = \"chemical\"\n",
    "database = \"chebi\"\n",
    "\n",
    "print(f\"Test model: {test_model_file}\")\n",
    "print(f\"Model directory: {model_dir}\")\n",
    "print(f\"LLM model: {llm_model}\")\n",
    "print(f\"Entity type: {entity_type}\")\n",
    "print(f\"Database: {database}\")\n",
    "print(f\"Max entities per model: {max_entities_per_model}\")\n",
    "print(f\"Number of models to test: {num_models_to_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Single Model Evaluation\n",
    "\n",
    "Test the evaluation of a single model using both the core interface and utils functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Single Model Evaluation\n",
      "----------------------------------------\n",
      "✓ Test model found: BIOMD0000000190.xml\n"
     ]
    }
   ],
   "source": [
    "print(\"Test 1: Single Model Evaluation\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Check if test model exists\n",
    "if os.path.exists(test_model_file):\n",
    "    print(f\"✓ Test model found: {test_model_file}\")\n",
    "else:\n",
    "    print(f\"✗ Test model not found: {test_model_file}\")\n",
    "    print(\"Please ensure the test model is available in the tests directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 16:35:00,525 - INFO - Starting annotation for model: test_models/BIOMD0000000190.xml\n",
      "2025-05-25 16:35:00,526 - INFO - Using LLM model: meta-llama/llama-3.3-70b-instruct:free\n",
      "2025-05-25 16:35:00,526 - INFO - Entity type: chemical, Database: chebi\n",
      "2025-05-25 16:35:00,526 - INFO - Step 1: Finding existing annotations...\n",
      "2025-05-25 16:35:00,537 - INFO - Found 11 entities with existing annotations\n",
      "2025-05-25 16:35:00,537 - INFO - Selected 10 entities for evaluation\n",
      "2025-05-25 16:35:00,537 - INFO - Step 3: Extracting model context...\n",
      "2025-05-25 16:35:00,578 - INFO - Extracted context for model: Model_1\n",
      "2025-05-25 16:35:00,578 - INFO - Step 4: Formatting LLM prompt...\n",
      "2025-05-25 16:35:00,608 - INFO - Step 5: Querying LLM (meta-llama/llama-3.3-70b-instruct:free)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.1 Testing core interface (annotate_single_model)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 16:35:01,941 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-25 16:35:15,369 - INFO - LLM response received in 14.76s\n",
      "2025-05-25 16:35:15,373 - INFO - Step 6: Parsing LLM response...\n",
      "2025-05-25 16:35:15,375 - INFO - Parsed synonyms for 11 entities\n",
      "2025-05-25 16:35:15,375 - INFO - Step 7: Searching chebi database...\n",
      "2025-05-25 16:35:16,803 - INFO - Database search completed in 1.43s\n",
      "2025-05-25 16:35:16,803 - INFO - Step 8: Generating recommendation table...\n",
      "2025-05-25 16:35:16,806 - INFO - Annotation completed in 16.28s\n",
      "2025-05-25 16:35:16,806 - INFO - Generated 30 recommendations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Core interface test successful\n",
      "  - Generated 30 recommendations\n",
      "  - Accuracy: 90.0%\n",
      "  - Annotation rate: 90.9%\n",
      "  - Total time: 16.28s\n",
      "  - LLM time: 14.76s\n",
      "  - Search time: 1.43s\n",
      "\n",
      "  Sample recommendations:\n",
      " id             display_name  annotation            annotation_label  match_score  existing\n",
      "SAM  S-adenosyl-L-methionine CHEBI:15414     S-adenosyl-L-methionine     0.666667         1\n",
      "SAM  S-adenosyl-L-methionine CHEBI:33442 (S)-S-adenosyl-L-methionine     0.333333         0\n",
      "SAM  S-adenosyl-L-methionine CHEBI:67040   S-adenosyl-L-methioninate     0.333333         0\n",
      "  A S-adenosylmethioninamine CHEBI:15625    S-adenosylmethioninamine     1.000000         1\n",
      "  P               Putrescine CHEBI:17148                  putrescine     1.000000         1\n"
     ]
    }
   ],
   "source": [
    "# Test using core interface (annotate_single_model)\n",
    "print(\"\\n1.1 Testing core interface (annotate_single_model)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    recommendations_df, metrics = annotate_single_model(\n",
    "        model_file=test_model_file,\n",
    "        llm_model=llm_model,\n",
    "        max_entities=max_entities_per_model,\n",
    "        entity_type=entity_type,\n",
    "        database=database\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Core interface test successful\")\n",
    "    print(f\"  - Generated {len(recommendations_df)} recommendations\")\n",
    "    print(f\"  - Accuracy: {metrics['accuracy']:.1%}\")\n",
    "    print(f\"  - Annotation rate: {metrics['annotation_rate']:.1%}\")\n",
    "    print(f\"  - Total time: {metrics['total_time']:.2f}s\")\n",
    "    print(f\"  - LLM time: {metrics['llm_time']:.2f}s\")\n",
    "    print(f\"  - Search time: {metrics['search_time']:.2f}s\")\n",
    "    \n",
    "    # Display sample recommendations\n",
    "    print(\"\\n  Sample recommendations:\")\n",
    "    sample_cols = ['id', 'display_name', 'annotation', 'annotation_label', 'match_score', 'existing']\n",
    "    print(recommendations_df[sample_cols].head().to_string(index=False))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Core interface test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_entities': 11,\n",
       " 'entities_with_predictions': 10,\n",
       " 'annotation_rate': 0.9090909090909091,\n",
       " 'total_predictions': 30,\n",
       " 'correct_matches': 9,\n",
       " 'accuracy': 0.9,\n",
       " 'total_time': 16.279825925827026,\n",
       " 'llm_time': 14.760626077651978,\n",
       " 'search_time': 1.4276001453399658}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 16:39:39,867 - INFO - Evaluating model: BIOMD0000000190.xml\n",
      "2025-05-25 16:39:39,878 - INFO - Evaluating 10 entities in BIOMD0000000190.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.2 Testing utils evaluation function\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 16:39:41,592 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM results saved to: results/llama-3.3-70b-instruct/BIOMD0000000190_llm_results.txt\n",
      "✓ Utils evaluation test successful\n",
      "  - Generated 10 result rows\n",
      "  - Average accuracy: 100.0%\n",
      "\n",
      "  Sample results:\n",
      "species_id                        display_name                                                                           synonyms_LLM                             predictions  accuracy  total_time\n",
      "       SAM                S-adenosylmethionine                                                   [S-adenosylmethionine, AdoMet, SAMe] [CHEBI:15414, CHEBI:33442, CHEBI:67040]         1   19.295151\n",
      "         A decarboxylated S-adenosylmethionine              [decarboxylated S-adenosylmethionine, S-adenosylmethioninamine, dcAdoMet]                           [CHEBI:15625]         1   19.295151\n",
      "         P                          putrescine                                    [putrescine, 1,4-diaminobutane, butane-1,4-diamine]             [CHEBI:17148, CHEBI:326268]         1   19.295151\n",
      "         S                            spermine [spermine, N,N'-bis(3-aminopropyl)propane-1,4-diamine, 1,12-diamino-4,9-diazadodecane]              [CHEBI:15746, CHEBI:45725]         1   19.295151\n",
      "         D                          spermidine            [spermidine, N-(3-aminopropyl)propane-1,4-diamine, 1,8-diamino-4-azaoctane]              [CHEBI:16610, CHEBI:57834]         1   19.295151\n",
      "\n",
      "  Formula-based metrics:\n",
      "species_id  recall_formula  precision_formula  recall_chebi  precision_chebi\n",
      "       SAM             1.0                1.0           1.0            0.333\n",
      "         A             1.0                1.0           1.0            1.000\n",
      "         P             1.0                1.0           1.0            0.500\n",
      "         S             1.0                1.0           1.0            0.500\n",
      "         D             1.0                1.0           1.0            0.500\n",
      "\n",
      "  LLM Results:\n",
      "  SAM: ['S-adenosylmethionine', 'AdoMet', 'SAMe']\n",
      "  A: ['decarboxylated S-adenosylmethionine', 'S-adenosylmethioninamine', 'dcAdoMet']\n",
      "  P: ['putrescine', '1,4-diaminobutane', 'butane-1,4-diamine']\n",
      "  Reason: The species names were matched to their standardized names based on the provided display names and c...\n",
      "\n",
      "  Match scores:\n",
      "  SAM: [1.0, 0.3333333333333333, 0.6666666666666666]\n",
      "  A: [1.0]\n",
      "  P: [1.0, 0.3333333333333333]\n",
      "\n",
      "  Existing annotation names:\n",
      "  SAM: S-adenosyl-L-methionine\n",
      "  A: S-adenosylmethioninamine\n",
      "  P: putrescine\n",
      "\n",
      "  Results saved to: ./results/single_model_test_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Test using utils evaluation function\n",
    "print(\"\\n1.2 Testing utils evaluation function\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    result_df = evaluate_single_model(\n",
    "        model_file=test_model_file,\n",
    "        llm_model=llm_model,\n",
    "        max_entities=max_entities_per_model,\n",
    "        entity_type=entity_type,\n",
    "        database=database,\n",
    "        save_llm_results=True,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    if result_df is not None:\n",
    "        print(f\"✓ Utils evaluation test successful\")\n",
    "        print(f\"  - Generated {len(result_df)} result rows\")\n",
    "        print(f\"  - Average accuracy: {result_df['accuracy'].mean():.1%}\")\n",
    "        \n",
    "        # Display sample results with updated column names\n",
    "        print(\"\\n  Sample results:\")\n",
    "        sample_cols = ['species_id', 'display_name', 'synonyms_LLM', 'predictions', 'accuracy', 'total_time']\n",
    "        available_cols = [col for col in sample_cols if col in result_df.columns]\n",
    "        print(result_df[available_cols].head().to_string(index=False))\n",
    "        \n",
    "        # Show formula-based metrics\n",
    "        print(\"\\n  Formula-based metrics:\")\n",
    "        formula_cols = ['species_id', 'recall_formula', 'precision_formula', 'recall_chebi', 'precision_chebi']\n",
    "        available_formula_cols = [col for col in formula_cols if col in result_df.columns]\n",
    "        print(result_df[available_formula_cols].head().to_string(index=False))\n",
    "        \n",
    "        # Show LLM results\n",
    "        print(\"\\n  LLM Results:\")\n",
    "        llm_cols = ['species_id', 'synonyms_LLM', 'reason']\n",
    "        available_llm_cols = [col for col in llm_cols if col in result_df.columns]\n",
    "        if available_llm_cols:\n",
    "            for idx, row in result_df[available_llm_cols].head(3).iterrows():\n",
    "                print(f\"  {row['species_id']}: {row.get('synonyms_LLM', 'N/A')}\")\n",
    "            if 'reason' in result_df.columns and not result_df['reason'].empty:\n",
    "                print(f\"  Reason: {result_df['reason'].iloc[0][:100]}...\")\n",
    "        \n",
    "        # Show match scores (updated from predictions_hits)\n",
    "        print(\"\\n  Match scores:\")\n",
    "        if 'match_score' in result_df.columns:\n",
    "            for idx, row in result_df[['species_id', 'match_score']].head(3).iterrows():\n",
    "                print(f\"  {row['species_id']}: {row['match_score']}\")\n",
    "        \n",
    "        # Show existing annotation names (now using proper ChEBI labels)\n",
    "        print(\"\\n  Existing annotation names:\")\n",
    "        if 'exist_annotation_name' in result_df.columns:\n",
    "            for idx, row in result_df[['species_id', 'exist_annotation_name']].head(3).iterrows():\n",
    "                print(f\"  {row['species_id']}: {row['exist_annotation_name']}\")\n",
    "        \n",
    "        # Save results\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        result_file = os.path.join(output_dir, \"single_model_test_results.csv\")\n",
    "        result_df.to_csv(result_file, index=False)\n",
    "        print(f\"\\n  Results saved to: {result_file}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"✗ Utils evaluation test failed: No results generated\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ Utils evaluation test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Batch Model Evaluation\n",
    "\n",
    "Test the evaluation of multiple models in a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 2: Batch Model Evaluation\n",
      "----------------------------------------\n",
      "✓ Model directory found: test_models\n",
      "  - Found 3 XML files\n",
      "  - Will test first 3 models\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest 2: Batch Model Evaluation\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Check if model directory exists\n",
    "if os.path.exists(model_dir):\n",
    "    model_files = [f for f in os.listdir(model_dir) if f.endswith('.xml')]\n",
    "    print(f\"✓ Model directory found: {model_dir}\")\n",
    "    print(f\"  - Found {len(model_files)} XML files\")\n",
    "    print(f\"  - Will test first {min(num_models_to_test, len(model_files))} models\")\n",
    "else:\n",
    "    print(f\"✗ Model directory not found: {model_dir}\")\n",
    "    print(\"Skipping batch evaluation test.\")\n",
    "    model_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 16:31:32,475 - INFO - Evaluating 3 models starting from index 1\n",
      "2025-05-25 16:31:32,476 - INFO - Evaluating 1/3: BIOMD0000000190.xml\n",
      "2025-05-25 16:31:32,476 - INFO - Evaluating model: BIOMD0000000190.xml\n",
      "2025-05-25 16:31:32,494 - INFO - Evaluating 10 entities in BIOMD0000000190.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2.1 Running batch evaluation\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 16:31:33,726 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-25 16:31:47,641 - INFO - Saved intermediate results to: results/batch_evaluation_results.csv_1.csv\n",
      "2025-05-25 16:31:47,642 - INFO - Evaluating 2/3: BIOMD0000000508.xml\n",
      "2025-05-25 16:31:47,642 - INFO - Evaluating model: BIOMD0000000508.xml\n",
      "2025-05-25 16:31:47,646 - INFO - Evaluating 5 entities in BIOMD0000000508.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM results saved to: results/llama-3.3-70b-instruct/BIOMD0000000190_llm_results.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 16:31:50,066 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-25 16:31:52,557 - INFO - Saved intermediate results to: results/batch_evaluation_results.csv_2.csv\n",
      "2025-05-25 16:31:52,557 - INFO - Evaluating 3/3: BIOMD0000000634.xml\n",
      "2025-05-25 16:31:52,557 - INFO - Evaluating model: BIOMD0000000634.xml\n",
      "2025-05-25 16:31:52,574 - INFO - Evaluating 5 entities in BIOMD0000000634.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM results saved to: results/llama-3.3-70b-instruct/BIOMD0000000508_llm_results.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 16:31:54,555 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-25 16:32:03,496 - INFO - Saved intermediate results to: results/batch_evaluation_results.csv_3.csv\n",
      "2025-05-25 16:32:03,498 - INFO - Saved final results to: results/batch_evaluation_results.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM results saved to: results/llama-3.3-70b-instruct/BIOMD0000000634_llm_results.txt\n",
      "✓ Batch evaluation successful\n",
      "  - Evaluated 3 models\n",
      "  - Generated 20 total result rows\n",
      "  - Average accuracy: 80.0%\n",
      "\n",
      "  Updated metrics summary:\n",
      "  - Average recall (formula): 0.800\n",
      "  - Average precision (formula): 0.750\n",
      "  - Average recall (ChEBI): 0.800\n",
      "  - Average precision (ChEBI): 0.424\n",
      "\n",
      "  Sample LLM results:\n",
      "    SAM: ['S-adenosylmethionine', 'AdoMet', 'SAMe']\n",
      "    A: ['S-adenosylmethioninamine', 'decarboxylated S-adenosylmethionine', 'dcAdoMet']\n",
      "    P: ['putrescine', '1,4-diaminobutane', 'butane-1,4-diamine']\n",
      "\n",
      "  Sample match scores:\n",
      "    SAM: [1.0, 0.3333333333333333, 0.6666666666666666]\n",
      "    A: [1.0]\n",
      "    P: [1.0, 0.3333333333333333]\n",
      "\n",
      "  Summary statistics:\n",
      "Number of models assessed: 3\n",
      "Number of models with predictions: 3\n",
      "Average accuracy (per model): 0.77\n",
      "Ave. total time (per model): 10.20\n",
      "Ave. total time (per element, per model): 1.53\n",
      "Ave. LLM time (per model): 9.76\n",
      "Ave. LLM time (per element, per model): 1.46\n",
      "Average number of predictions per species: 2.15\n"
     ]
    }
   ],
   "source": [
    "# Run batch evaluation if models are available\n",
    "if model_files:\n",
    "    print(\"\\n2.1 Running batch evaluation\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        batch_results_df = evaluate_models_in_folder(\n",
    "            model_dir=model_dir,\n",
    "            num_models=min(num_models_to_test, len(model_files)),\n",
    "            llm_model=llm_model,\n",
    "            max_entities=max_entities_per_model,\n",
    "            entity_type=entity_type,\n",
    "            database=database,\n",
    "            save_llm_results=True,\n",
    "            output_dir=output_dir,\n",
    "            output_file=\"batch_evaluation_results.csv\",\n",
    "            start_at=1\n",
    "        )\n",
    "        \n",
    "        if not batch_results_df.empty:\n",
    "            print(f\"✓ Batch evaluation successful\")\n",
    "            print(f\"  - Evaluated {batch_results_df['model'].nunique()} models\")\n",
    "            print(f\"  - Generated {len(batch_results_df)} total result rows\")\n",
    "            print(f\"  - Average accuracy: {batch_results_df['accuracy'].mean():.1%}\")\n",
    "            \n",
    "            # Show updated metrics\n",
    "            print(\"\\n  Updated metrics summary:\")\n",
    "            if 'recall_formula' in batch_results_df.columns:\n",
    "                print(f\"  - Average recall (formula): {batch_results_df['recall_formula'].mean():.3f}\")\n",
    "                print(f\"  - Average precision (formula): {batch_results_df['precision_formula'].mean():.3f}\")\n",
    "            print(f\"  - Average recall (ChEBI): {batch_results_df['recall_chebi'].mean():.3f}\")\n",
    "            print(f\"  - Average precision (ChEBI): {batch_results_df['precision_chebi'].mean():.3f}\")\n",
    "            \n",
    "            # Show sample of LLM results\n",
    "            print(\"\\n  Sample LLM results:\")\n",
    "            if 'synonyms_LLM' in batch_results_df.columns:\n",
    "                for idx, row in batch_results_df[['species_id', 'synonyms_LLM']].head(3).iterrows():\n",
    "                    print(f\"    {row['species_id']}: {row['synonyms_LLM']}\")\n",
    "            \n",
    "            # Show match scores instead of predictions_hits\n",
    "            print(\"\\n  Sample match scores:\")\n",
    "            if 'match_score' in batch_results_df.columns:\n",
    "                for idx, row in batch_results_df[['species_id', 'match_score']].head(3).iterrows():\n",
    "                    print(f\"    {row['species_id']}: {row['match_score']}\")\n",
    "            \n",
    "            # Print summary statistics\n",
    "            print(\"\\n  Summary statistics:\")\n",
    "            print_evaluation_results(os.path.join(output_dir, \"batch_evaluation_results.csv\"))\n",
    "            \n",
    "        else:\n",
    "            print(f\"✗ Batch evaluation failed: No results generated\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Batch evaluation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"\\nSkipping batch evaluation - no model directory available\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
